---
title: "ANOVA Exploratory Data Analysis"
output: html_document
---

{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(car)
library(ggrain)
library(multcomp)

# Create example dataset for demonstrations
df <- PlantGrowth %>% rename(response = weight)

Document 1: Pre-Modeling ANOVA EDA

Why Check Your Data Before Modeling

Always examine your data before you run ANOVA. Plots show you patterns that p-values can't reveal. Document what you find before you proceed to formal testing.

Check Sample Size and Balance

Assess your group sizes

Verify that your design has adequate power and balanced groups.

{r sample-size}
# Check group sizes and balance
df_a <- df %>% 
  group_by(group) %>% 
  summarize(
    mean = mean(response), 
    sd = sd(response), 
    n = n()
    ) %>% 
  ungroup() %>% 
  mutate(ratio = max(n) / min(n))
df_a %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Questions to answer:

Are your groups balanced? Unbalanced designs affect power and the calculation of Type II/III sums of squares.

Do you have major imbalances? Look for maximum-to-minimum ratios exceeding 4:1.

Are any groups too small? Groups with fewer than 10 observations may lack adequate power.

Is your total sample size adequate? Run a power analysis to verify you can detect meaningful effects.

{r power-analysis}
# Power analysis for ANOVA
# https://aaroncaldwell.us/SuperpowerBook/one-way-anova.html
design_result <- Superpower::ANOVA_design(
  design = paste0(nrow(df_a), "b"),
  n = df_a$n,
  mu = df_a$mean,
  sd = df_a$sd
  )
simulation_result <- Superpower::ANOVA_power(
  design_result,
  alpha_level = 0.05,
  nsims = 1000,
  verbose = FALSE
  )
simulation_result

Check Distributions

Assess shape within each group

Check whether your data are symmetric or skewed.

{r distribution-summary}
# Compare means and medians by group
df %>% 
  group_by(group) %>% 
  summarize(
    mean = mean(response), 
    median = median(response), 
    diff = mean - median, 
    skewness = diff / sd(response)
    )  %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Questions to answer:

Is the distribution symmetric or skewed? Compare means and mediansâ€”similar values suggest symmetry.

For symmetric data: Is the shape mound-shaped or uniform? Are the tails thin or thick?

For skewed data: Note the direction (right-skewed = positive skew; left-skewed = negative skew).

{r histograms}
# Histogram + Density by group
ggplot(df, aes(x = response)) + 
  geom_density(fill = "steelblue", alpha = 0.7) + 
  geom_histogram(bins = 7, fill = "steelblue", alpha = 0.7) +
  facet_wrap(~group) + 
  theme_minimal()

Assess normality within each group

Check whether your data meet the normality assumption.

{r qq-plots}
# QQ plots by group
ggplot(df, aes(sample = response)) + 
  stat_qq() + stat_qq_line(color = "red") + 
  facet_wrap(~group) + 
  theme_minimal()

{r shapiro-test}
# Formal normality tests by group
df %>% 
  group_by(group) %>% 
  summarize(shapiro_p = shapiro.test(response)$p.value) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Key points:

Create QQ plots for each group. Points falling along the diagonal indicate normality.

Compare histogram shapes to a normal distribution. Look for bell-shaped curves centered on the mean.

For small samples: Assess whether departures are severe enough to matter given your limited data.

Small departures from normality matter less with large sample sizes (Central Limit Theorem). Severe departures with small samples require remedial action.

Compare Central Tendency

Assess location differences across groups

Determine whether your groups differ in meaningful ways.

{r central-tendency}
# Summary statistics by group
df %>% 
  group_by(group) %>% 
  summarize(
    mean = mean(response), 
    median = median(response), 
    sd = sd(response), 
    n = n()
    ) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

{r boxplot}
# Quick visual comparison
ggplot(df, aes(x = group, y = response, fill = group)) + 
  geom_boxplot(alpha = 0.7) + theme_minimal() + 
  theme(legend.position = "none")

Questions to answer:

How large are differences between means and medians? Look for both statistical and practical significance.

Is separation visually apparent? Groups should show clear differences if ANOVA will detect them.

What's the magnitude of differences? Consider fold-changes and standardized effect sizes.

Which factor levels drive observed differences? Identify specific groups that appear distinct.

{r effect-size}
# Calculate effect sizes (Cohen's f)
grand_mean <- mean(df$response)
group_means <- df %>% group_by(group) %>% summarize(m = mean(response))
sd_pooled <- sqrt(mean(tapply(df$response, df$group, var)))
cohens_f <- sd(group_means$m) / sd_pooled
cohens_f

Compare Variability

Check spread within groups

Assess how much observations vary within each group.

{r variability}
# Compare variability across groups
df %>% 
  group_by(group) %>% 
  summarize(
    sd = sd(response), 
    var = var(response), 
    range = max(response) - min(response)
    ) %>%
  mutate(max_sd_ratio = max(sd) / min(sd)) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Questions to answer:

Is variability wide or narrow? Compare ranges and standard deviations.

Are ranges roughly similar across groups? Similar ranges suggest similar variances.

Check homogeneity across groups

Verify that the homogeneity of variance (HOV) assumption holds.

{r levene-test}
# Levene's test for homogeneity of variance
leveneTest(response ~ group, data = df) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

{r brown-forsythe}
# Brown-Forsythe test (more robust to non-normality)
leveneTest(response ~ group, data = df, center = median) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Key points:

Do standard deviations appear similar across groups? Use a rough 2:1 maximum-to-minimum ratio as a guideline.

Does any group show both the lowest and highest value? This pattern may indicate a response to observation (Hawthorne effect) rather than treatment.

Consider sample size effects. Large samples make tests sensitive to trivial violations. Small samples may miss real violations.

Identify Outliers

Find potential outliers

Check for extreme values in any group.

{r outliers}
# Identify outliers using IQR method
df %>% 
  group_by(group) %>% 
  mutate(
    Q1 = quantile(response, 0.25), 
    Q3 = quantile(response, 0.75),
    IQR = Q3 - Q1,
    lower_bound = Q1 - 1.5 * IQR,
    upper_bound = Q3 + 1.5 * IQR,
    is_outlier = response < lower_bound | response > upper_bound
    ) %>%
  filter(is_outlier) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

Questions to answer:

Do outliers appear in any groups? Look for points far from the group center.

Are outliers extreme values consistent with the distribution, or are they errors? Consider whether outliers represent real but rare observations or data entry mistakes.

You'll assess whether these outliers influence your results after you fit your model.

Verify Independence

Check that observations are independent

Verify that the independence assumption holds.

{r independence-check}
# Check for patterns by row number (collection order)
df_indexed <- df %>% mutate(order = row_number())
ggplot(df_indexed, aes(x = order, y = response, color = group)) + 
  geom_point() + 
  geom_smooth(se = FALSE) + 
  theme_minimal()

{r autocorrelation}
# Check autocorrelation (if data have time ordering)
acf(df$response, main = "Autocorrelation Function")

Questions to answer:

Were observations collected independently? Consider whether one observation could affect another.

Is there clustering in your design? Repeated measures or nested designs violate independence.

Does temporal or spatial ordering matter? Sequential observations may be correlated.

If independence is violated, you need mixed models or repeated measures ANOVA rather than standard ANOVA.

Check for Interactions (Factorial Designs Only)

If you have two or more factors, assess whether they interact.

{r factorial-data, include=FALSE}
# Create factorial example
df_factorial <- expand_grid(
  factor_A = c("A1", "A2"),
  factor_B = c("B1", "B2", "B3"),
  rep = 1:20
) %>%
  mutate(
    response = 100 
    + 10 * (factor_A == "A2") 
    + 5 * (factor_B == "B2") 
    + 15 * (factor_B == "B3") 
    + 20 * (factor_A == "A2" & factor_B == "B3") 
    +  rnorm(n(), 0, 8)) # Interaction

Examine two-way interactions

Create interaction plots with one factor on the x-axis and separate lines for each level of the second factor.

{r interaction-plot}
# Interaction plot
ggplot(df_factorial, 
       aes(x = factor_B, y = response, color = factor_A, group = factor_A)) +
  stat_summary(fun = mean, geom = "line", linewidth = 1) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  theme_minimal() + 
  labs(title = "Interaction Plot")

{r interaction-plot-alt}
# Alternative using base R
with(df_factorial, interaction.plot(factor_B, factor_A, response))

Key patterns:

Parallel lines indicate no interaction. Factors affect the response independently.

Converging lines suggest ordinal interaction. One factor's effect diminishes at certain levels of the other factor.

Crossing lines indicate disordinal interaction. The direction of one factor's effect reverses depending on the other factor's level.

Choose Your Visualizations

Use plots that show multiple features

Raincloud plots capture multiple features simultaneously:

{r raincloud}
# Raincloud plot (requires ggrain package)
ggplot(df, aes(x = group, y = response, fill = group)) +
  geom_rain(alpha = 0.5) +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs(title = "Raincloud Plot: Distribution + Points + Boxplot")

Other useful plots:

{r violin-plot}
# Violin plot with points
ggplot(df, aes(x = group, y = response, fill = group)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(width = 0.1, alpha = 0.3) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +
  theme_minimal() + theme(legend.position = "none")

Decide Your Analysis Approach

Use what you found in your EDA to choose the appropriate analysis.

If HOV is violated

{r welch-anova}
# Welch's ANOVA (doesn't assume equal variances)
oneway.test(response ~ group, data = df, var.equal = FALSE)

{r log-transform}
# Try log transformation if variance increases with mean
leveneTest(
  log_response ~ group, 
  data = df %>% mutate(log_response = log(response))) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

If normality is violated

{r kruskal-wallis}
# Kruskal-Wallis test (non-parametric alternative)
kruskal.test(response ~ group, data = df)

If severe outliers exist

{r robust-anova}
# Trimmed means ANOVA (robust to outliers)
library(WRS2)

{r }
t1way(response ~ group, data = df, tr = 0.1)  # 10% trimming

Transform Data if Needed

Apply transformations to skewed data

{r transformations}
# Common transformations for positive skew
df_transform <- df %>% 
  mutate(
    log_y = log(response),
    sqrt_y = sqrt(response),
    inv_y = 1/response
  )

# Check which transformation improves normality
df_transform %>% 
  pivot_longer(cols = c(response, log_y, sqrt_y, inv_y)) %>%
  ggplot(aes(sample = value)) +
  stat_qq() + stat_qq_line(color = "red") +
  facet_wrap(~name, scales = "free") + theme_minimal()

{r box-cox}
# Box-Cox transformation (finds optimal lambda)
library(MASS)
bc <- boxcox(response ~ group, data = as.data.frame(df))
lambda <- bc$x[which.max(bc$y)]
df <- df %>% 
  mutate(boxcox_y = (response^lambda - 1) / lambda)%>% 
  mutate(lambda = lambda)
df

Document 2: Post-Modeling ANOVA EDA

Why Check Model Diagnostics

After you fit your ANOVA model, examine residuals to verify that your model meets assumptions. These checks reveal problems that raw data exploration might miss.

{r fit-model}
# Fit ANOVA model
model <- lm(response ~ group, data = df)
anova_result <- anova(model)
anova_result
broom::tidy(anova_result)

Check Residual Normality

Examine the overall model residuals

Check whether your model residuals are approximately normal.

{r residual-qq}
# QQ plot of residuals
ggplot(data.frame(residuals = residuals(model)), aes(sample = residuals)) +
  stat_qq() + 
  stat_qq_line(color = "red") + 
  theme_minimal() +
  labs(title = "QQ Plot of Residuals")

{r residual-histogram}
# Density + Histogram of residuals
ggplot(data.frame(residuals = residuals(model)), aes(x = residuals)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_histogram(bins = 7, fill = "steelblue", alpha = 0.7) +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() + 
  labs(title = "Distribution of Residuals")

{r residual-normality-test}
# Shapiro-Wilk test on residuals
shapiro.test(residuals(model))

Interpret normality violations

{r residuals-by-group}
# Check residual normality within each group
df %>% 
  mutate(residuals = residuals(model)) %>%
  ggplot(aes(sample = residuals)) +
  stat_qq() + 
  stat_qq_line(color = "red") +
  facet_wrap(~group) + 
  theme_minimal()

Key points:

With n > 30 per group: ANOVA is robust to moderate departures from normality.

With n < 30 per group: Even mild departures may affect your results.

Severe departures at any sample size: Consider transformations or non-parametric alternatives.

Check Homogeneity of Variance

Examine residual plots

Create plots to assess whether variance is constant.

{r residual-fitted}
# Residuals vs fitted values
ggplot(
  data.frame(fitted = fitted(model), residuals = residuals(model)),
  aes(x = fitted, y = residuals)
  ) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(se = FALSE) +
  theme_minimal() + 
  labs(title = "Residuals vs Fitted Values")

{r residuals-by-group-plot}
# Residuals by group
df %>% 
  mutate(residuals = residuals(model)) %>%
  ggplot(aes(x = group, y = residuals, fill = group)) +
  geom_boxplot(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() + 
  theme(legend.position = "none")

{r scale-location}
# Scale-Location plot (check for homoscedasticity)
plot(model, which = 3)

Assess Outlier Influence

Calculate influence diagnostics

Determine whether outliers you identified earlier affect your conclusions.

{r cooks-distance}
# Cook's distance
cooks_d <- cooks.distance(model)
influential <- which(cooks_d > 4/nrow(df))
data.frame(observation = influential, cooks_d = cooks_d[influential]) %>% 
  mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

{r cooks-plot}
# Plot Cook's distance
ggplot(
  data.frame(index = 1:length(cooks_d), cooks_d = cooks_d),
  aes(x = index, y = cooks_d)
  ) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_hline(yintercept = 4/nrow(df), color = "red", linetype = "dashed") +
  theme_minimal() + 
  labs(title = "Cook's Distance")

{r dffits}
# DFFITS
dffits_val <- dffits(model)
threshold <- 2 * sqrt(length(coef(model)) / nrow(df))
influential_dffits <- which(abs(dffits_val) > threshold)
data.frame(
  observation = influential_dffits, 
  dffits = dffits_val[influential_dffits]
  ) %>% mutate(across(where(is.double), ~round(.x, 4))) %>% 
  DT::datatable()

{r influence-plot}
# Comprehensive influence plot
plot(model, which = 5)  # Residuals vs Leverage

Investigate influential points

{r sensitivity-analysis}
# Compare models with and without influential points
if(length(influential) > 0) {
  model_reduced <- lm(response ~ group, data = df[-influential, ])
  comparison <- data.frame(
    model = c("Full", "Without influential"),
    F_statistic = c(anova_result$`F value`[1], 
                    anova(model_reduced)$`F value`[1]),
    p_value = c(anova_result$`Pr(>F)`[1], 
                anova(model_reduced)$`Pr(>F)`[1])
  )
  comparison %>% 
    mutate(across(where(is.double), ~round(.x, 4))) %>% 
    DT::datatable()
}

Check for Systematic Patterns

Look for residual patterns

Examine residual plots for patterns that indicate model problems.

{r residual-patterns}
# Residuals vs order (if data have temporal ordering)
df_diagnostics <- df %>%
  mutate(
    residuals = residuals(model),
    fitted = fitted(model),
    order = row_number()
  )

ggplot(df_diagnostics, aes(x = order, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() + 
  labs(title = "Residuals vs Order")

{r diagnostic-panel}
# Standard diagnostic plots panel
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1, 1))

Verify Your Decisions

Confirm your pre-modeling choices

{r compare-approaches}
# Compare standard ANOVA vs Welch's ANOVA
standard <- anova(model)
welch <- oneway.test(response ~ group, data = df, var.equal = FALSE)

paste0("Standard ANOVA p-value:", standard$`Pr(>F)`[1])
paste0("Welch's ANOVA p-value:", welch$p.value)

{r compare-transformations}
# Compare original vs transformed data
model_original <- lm(response ~ group, data = df)
model_log <- lm(log(response) ~ group, data = df)

paste0("Original data - Levene's test p-value:", leveneTest(response ~ group, data = df)$`Pr(>F)`[1])
paste0("Log-transformed - Levene's test p-value:", leveneTest(log(response) ~ group, data = df)$`Pr(>F)`[1])

Report Your Diagnostics

Create a diagnostic summary

{r diagnostic-summary}
# Comprehensive diagnostic summary
diagnostic_summary <- data.frame(
  Test = c("Normality (Shapiro-Wilk)", 
           "Homogeneity of Variance (Levene)",
           "Number of influential points (Cook's D > 4/n)",
           "Max Cook's Distance"),
  Result = c(
    shapiro.test(residuals(model))$p.value,
    leveneTest(response ~ group, data = df)$`Pr(>F)`[1],
    sum(cooks_d > 4/nrow(df)),
    max(cooks_d)
  )
)
diagnostic_summary %>% 
    mutate(across(where(is.double), ~round(.x, 4))) %>% 
    DT::datatable()

Create publication-ready diagnostic figure

{r publication-diagnostics}
library(patchwork)

p1 <- ggplot(data.frame(residuals = residuals(model)), 
             aes(sample = residuals)) +
  stat_qq() + stat_qq_line(color = "red") + 
  theme_minimal() + labs(title = "A. Normal Q-Q Plot")

p2 <- ggplot(data.frame(fitted = fitted(model), 
                        residuals = residuals(model)),
             aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  geom_smooth(se = FALSE) +
  theme_minimal() + labs(title = "B. Residuals vs Fitted")

p1 + p2

Complete Workflow Example

{r complete-workflow}
# Complete ANOVA workflow with diagnostics

# 1. Check assumptions pre-modeling

# PRE-MODELING CHECKS
cat("PRE-MODELING CHECKS\n")
# Sample sizes 
table(df$group)

# Descriptive statistics
cat("Descriptive statistics\n")
df %>% 
  group_by(group) %>% 
  summarize(mean = mean(response), sd = sd(response))

# 2. Fit model
model <- lm(response ~ group, data = df)

# 3. Check diagnostics post-modeling

# POST-MODELING DIAGNOSTICS
cat("POST-MODELING CHECKS")
paste0("Normality test p-value:", shapiro.test(residuals(model))$p.value)
paste0("Levene's test p-value:", leveneTest(response ~ group, data = df)$`Pr(>F)`[1])
paste0("Influential points:", sum(cooks.distance(model) > 4/nrow(df)))

# 4. Report results

# ANOVA RESULTS
cat("ANOVA RESULTS\n")
print(anova(model))

# 5. Post-hoc comparisons if significant
if(anova(model)$`Pr(>F)`[1] < 0.05) {
  # POST-HOC COMPARISONS (Tukey HSD)
  cat("POST-HOC COMPARISONS (Tukey HSD)\n")
  print(TukeyHSD(aov(response ~ group, data = df)))
}

